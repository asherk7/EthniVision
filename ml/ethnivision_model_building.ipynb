{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Ethnicities, Age, and Gender with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "1. *Introduction*\n",
    "2. *Data Preperation*\n",
    "    * 2.1 Data Cleaning/Preparation\n",
    "    * 2.2 Data Loading\n",
    "    * 2.3 Data Preprocessing\n",
    "    * 2.4 Data Exploration\n",
    "3. *Model Architecture*\n",
    "    * 3.1 Neural Network Design\n",
    "    * 3.2 Model Compilation\n",
    "4. *Model Training*\n",
    "    * 4.1 Training Process\n",
    "5. *Model Evaluation*\n",
    "    * 5.1 Model Performance\n",
    "    * 5.2 Confusion Matrix\n",
    "6. *Model Deployment*\n",
    "    * 6.1 Model Saving\n",
    "7. *Conclusion*\n",
    "8. *References*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction:\n",
    "In this notebook, we will explore the process of building a neural network model to predict ethnicities, age, and gender based on certain features. Predicting ethnicities, age, and gender can have various applications, including demographic analysis, social studies, and more. We will follow a step-by-step approach, covering data preprocessing, model architecture design, training, evaluation, and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Cleaning/Preparation\n",
    "\n",
    "The dataset from FairFace is split between a training set and a validation set, lacking the test set. In this section, I will:\n",
    "- Combine all the images into one folder\n",
    "- Rename the images starting from 1 to the total number of images\n",
    "- Compile all the csv files into one with the new names\n",
    "- Split the images and csv into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the training images to the all images folder\n",
    "temp_train_path = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\temp\\\\train\"\n",
    "all_images = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\temp\\\\all\"\n",
    "\n",
    "train_files = os.listdir(temp_train_path)\n",
    "\n",
    "# Moving the training images to the all images folder\n",
    "for file_name in train_files:\n",
    "    train_path = os.path.join(temp_train_path, file_name)\n",
    "    all_path = os.path.join(all_images, file_name)\n",
    "    shutil.move(train_path, all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the contents of the train csv to a general csv file\n",
    "# Changing the file column to only include the name of the file\n",
    "temp_train_csv = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\temp_csv\\\\images_train.csv\"\n",
    "general_csv = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\temp_csv\\\\all.csv\"\n",
    "\n",
    "# getting the data from the csv without the extra path on the filename\n",
    "new_rows = []\n",
    "with open(temp_train_csv, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        modified_row = [row[0].split('/')[-1]] + row[1:]\n",
    "        new_rows.append(modified_row)\n",
    "\n",
    "# writing the data back into the csv\n",
    "with open(general_csv, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86745"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting a variable for the name of the last image in train\n",
    "img_num = len(os.listdir(all_images)) + 1\n",
    "img_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the validation images and csv also start from 1, we will have to rename the images starting from the end of the training images, while also renaming the file column in the val csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the validation images\n",
    "temp_val_path = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\temp\\\\val\"\n",
    "all_images = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\temp\\\\all\"\n",
    "temp_val_csv = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\temp_csv\\\\images_val.csv\"\n",
    "\n",
    "# getting our images without the extra path\n",
    "image_data = []\n",
    "with open(temp_val_csv, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        modified_row = [row[0].split('/')[-1]] + row[1:]\n",
    "        image_data.append(modified_row)\n",
    "\n",
    "# get rid of the header row\n",
    "image_data = image_data[1:]\n",
    "\n",
    "# changing the name of the images and the csv data and moving them\n",
    "for index, image in enumerate(image_data, start=img_num):\n",
    "    new_image_name = str(index) + \".jpg\"\n",
    "    new_image_path = os.path.join(all_images, new_image_name)\n",
    "    old_image_path = os.path.join(temp_val_path, image[0])\n",
    "    shutil.move(old_image_path, new_image_path)\n",
    "\n",
    "    for i, row in enumerate(image_data):\n",
    "        if (row[0] == image[0]):\n",
    "            image_data[i][0] = new_image_name\n",
    "            break\n",
    "\n",
    "# writing the new data into the csv file\n",
    "with open(temp_val_csv, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the val csv with the general csv\n",
    "general_csv = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\temp_csv\\\\all.csv\"\n",
    "temp_val_csv = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\temp_csv\\\\images_val.csv\"\n",
    "\n",
    "# getting the data from general csv\n",
    "rows = []\n",
    "with open(general_csv, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "# adding the val data to the data we already have\n",
    "with open(temp_val_csv, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "# writing the data to the general csv\n",
    "with open(general_csv, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to split the images and csv into a 70-15-15 split for training, validation, and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_source = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\all\"\n",
    "csv_source = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\all.csv\"\n",
    "image_destination = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\images\"\n",
    "csv_destination = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\csv\"\n",
    "\n",
    "# creating the directories\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "for split in splits:\n",
    "    os.mkdir(os.path.join(image_destination, split))\n",
    "\n",
    "# getting the data from the csv file\n",
    "csv_data = []\n",
    "header = []\n",
    "with open(csv_source, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    header = reader.__next__() # getting the header and skipping it\n",
    "    for row in reader:\n",
    "        csv_data.append(row)\n",
    "\n",
    "random.shuffle(csv_data)\n",
    "\n",
    "# splitting the data into train, val, test\n",
    "total_images = len(csv_data)\n",
    "train_size = int(0.70*total_images)\n",
    "val_size = int(0.15*total_images)\n",
    "\n",
    "train_data = csv_data[ : train_size]\n",
    "val_data = csv_data[train_size : train_size + val_size]\n",
    "test_data = csv_data[train_size + val_size : ]\n",
    "\n",
    "# moving the images and creating the csv files\n",
    "for split, data in zip(splits, [train_data, val_data, test_data]):\n",
    "    for row in data:\n",
    "        image_path = os.path.join(image_source, row[0])\n",
    "        split_destination = os.path.join(image_destination, split)\n",
    "        shutil.move(image_path, split_destination)\n",
    "\n",
    "    csv_path = os.path.join(csv_destination, split + \".csv\")\n",
    "    with open(csv_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(header)\n",
    "        for row in data:\n",
    "            modified_row = [os.path.join(split, row[0])] + row[1:]\n",
    "            writer.writerow(modified_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68388 14654 14656\n",
      "0\n",
      "97698\n",
      "68388\n",
      "14654\n",
      "14656\n",
      "97698\n"
     ]
    }
   ],
   "source": [
    "# making sure the data was moved properly\n",
    "train_dir = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\images\\\\train\"\n",
    "val_dir = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\images\\\\val\"\n",
    "test_dir = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\images\\\\test\"\n",
    "image_source = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\all\"\n",
    "train_csv = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\csv\\\\train.csv\"\n",
    "val_csv = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\csv\\\\val.csv\"\n",
    "test_csv = \"C:\\\\Users\\\\mashe\\\\Downloads\\\\csv\\\\test.csv\"\n",
    "\n",
    "train_len = len(os.listdir(train_dir))\n",
    "val_len = len(os.listdir(val_dir))\n",
    "test_len = len(os.listdir(test_dir))\n",
    "\n",
    "print(train_len, val_len, test_len)\n",
    "print(len(os.listdir(image_source))) # confirms data was moved\n",
    "print(train_len + val_len + test_len)\n",
    "\n",
    "counter = 0\n",
    "with open(train_csv, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        counter += 1\n",
    "print(counter)\n",
    "\n",
    "counter1 = 0\n",
    "with open(val_csv, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        counter1 += 1\n",
    "print(counter1)\n",
    "\n",
    "counter2 = 0\n",
    "with open(test_csv, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        counter2 += 1\n",
    "print(counter2)\n",
    "\n",
    "print(counter + counter1 + counter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned up our data, we can start loading it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the dlib library to detect and crop the faces, and store these cropped faces for training, validation, and testing later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_dlib_pybind11.cnn_face_detection_model_v1 at 0x21a626266f0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_detector = dlib.cnn_face_detection_model_v1('dlib_models/mmod_human_face_detector.dat')\n",
    "face_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Neural Network Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Model Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was retrieved from the FairFace study.\n",
    "\n",
    "Karkkainen, Kimmo and Joo, Jungseock. (2021). FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation. Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 1548-1558. 10.1109/WACV48630.2021.00159"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
